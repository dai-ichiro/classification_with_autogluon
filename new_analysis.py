import json
import pandas as pd
import numpy as np
from collections import defaultdict
import statistics

class HPODataExtractor:
    """HPOå®Ÿé¨“çµæœã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºãƒ»åˆ†æã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, json_file_path):
        """
        Args:
            json_file_path (str): å®Ÿé¨“çµæœJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.json_file_path = json_file_path
        self.data = None
        self.trials_df = None
        
    def load_data(self):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.json_file_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            print(f"âœ“ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {self.json_file_path}")
            return True
        except Exception as e:
            print(f"âœ— ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return False
    
    def extract_trials_data(self):
        """å…¨è©¦è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦DataFrameã«å¤‰æ›"""
        if not self.data:
            print("ã¾ãšload_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return None
        
        trials = []
        
        for i, trial_data in enumerate(self.data['trial_data']):
            try:
                # è¨­å®šãƒ‡ãƒ¼ã‚¿ã¨çµæœãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ã«ãƒ‘ãƒ¼ã‚¹
                trial_config = json.loads(trial_data[0])
                trial_results = json.loads(trial_data[1])
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’ç¢ºèªï¼ˆval_roc_aucã¾ãŸã¯val_accuracyã®ã©ã¡ã‚‰ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ï¼‰
                last_result = trial_results['last_result']
                metric_name = 'val_roc_auc' if 'val_roc_auc' in last_result else 'val_accuracy'
                
                # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                trial_info = {
                    'trial_id': trial_config['trial_id'],
                    'status': trial_config['status'],
                    
                    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                    'learning_rate': trial_config['config']['optim.lr'],
                    'optimizer': trial_config['config']['optim.optim_type'],
                    'batch_size': trial_config['config']['env.batch_size'],
                    'max_epochs': trial_config['config']['optim.max_epochs'],
                    'model_name': trial_config['config']['model.timm_image.checkpoint_name'],
                    
                    # çµæœï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹åã«å¿œã˜ã¦å‹•çš„ã«è¨­å®šï¼‰
                    'final_metric': last_result[metric_name],
                    'training_time_s': last_result['time_total_s'],
                    'final_iteration': last_result['training_iteration'],
                    'metric_name': metric_name,
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±è¨ˆ
                    'avg_metric': trial_results['metric_analysis'][metric_name]['avg'],
                    'max_metric': trial_results['metric_analysis'][metric_name]['max'],
                    'min_metric': trial_results['metric_analysis'][metric_name]['min'],
                    
                    # æ™‚é–“çµ±è¨ˆ
                    'avg_time_per_iter': trial_results['metric_analysis']['time_this_iter_s']['avg'],
                    'max_time_per_iter': trial_results['metric_analysis']['time_this_iter_s']['max'],
                    'min_time_per_iter': trial_results['metric_analysis']['time_this_iter_s']['min'],
                }
                
                trials.append(trial_info)
                
            except Exception as e:
                print(f"è©¦è¡Œ {i} ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        self.trials_df = pd.DataFrame(trials)
        print(f"âœ“ {len(trials)} è©¦è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
        return self.trials_df
    
    def get_summary(self):
        """å®Ÿé¨“ã®è¦ç´„çµ±è¨ˆã‚’è¡¨ç¤º"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’å–å¾—
        metric_name = self.trials_df['metric_name'].iloc[0] if len(self.trials_df) > 0 else 'unknown'
        
        print("\n" + "="*60)
        print("HPOå®Ÿé¨“çµæœã‚µãƒãƒªãƒ¼")
        print("="*60)
        print(f"è©¦è¡Œæ•°: {len(self.trials_df)}")
        print(f"è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {metric_name}")
        
        if 'start_time' in self.data.get('stats', {}):
            print(f"å®Ÿé¨“é–‹å§‹æ™‚é–“: {self.data['stats']['start_time']}")
        
        if '_total_time' in self.data.get('runner_data', {}):
            total_time = self.data['runner_data']['_total_time']
            print(f"ç·å®Ÿè¡Œæ™‚é–“: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†)")
        
        print(f"\nğŸ“Š {metric_name}ã‚¹ã‚³ã‚¢çµ±è¨ˆ:")
        print(f"  æœ€é«˜: {self.trials_df['final_metric'].max():.4f}")
        print(f"  æœ€ä½: {self.trials_df['final_metric'].min():.4f}")
        print(f"  å¹³å‡: {self.trials_df['final_metric'].mean():.4f}")
        print(f"  æ¨™æº–åå·®: {self.trials_df['final_metric'].std():.4f}")
        print(f"  ä¸­å¤®å€¤: {self.trials_df['final_metric'].median():.4f}")
        
        print("\nâ±ï¸  å­¦ç¿’æ™‚é–“çµ±è¨ˆ:")
        print(f"  æœ€é•·: {self.trials_df['training_time_s'].max():.2f}ç§’")
        print(f"  æœ€çŸ­: {self.trials_df['training_time_s'].min():.2f}ç§’")
        print(f"  å¹³å‡: {self.trials_df['training_time_s'].mean():.2f}ç§’")
        
        print("\nğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ:")
        print(f"  ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {self.trials_df['optimizer'].value_counts().to_dict()}")
        print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {sorted(self.trials_df['batch_size'].unique())}")
        print(f"  å­¦ç¿’ç‡ç¯„å›²: {self.trials_df['learning_rate'].min():.6f} - {self.trials_df['learning_rate'].max():.6f}")
        print(f"  ã‚¨ãƒãƒƒã‚¯æ•°ç¯„å›²: {self.trials_df['max_epochs'].min()} - {self.trials_df['max_epochs'].max()}")
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥çµ±è¨ˆ
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«åˆ¥çµ±è¨ˆ:")
        for model in self.trials_df['model_name'].unique():
            model_data = self.trials_df[self.trials_df['model_name'] == model]
            print(f"  {model}:")
            print(f"    è©¦è¡Œæ•°: {len(model_data)}")
            print(f"    å¹³å‡{metric_name}: {model_data['final_metric'].mean():.4f}")
            print(f"    æœ€é«˜{metric_name}: {model_data['final_metric'].max():.4f}")
    
    def get_best_trial(self):
        """æœ€é«˜æ€§èƒ½ã®è©¦è¡Œã‚’è¡¨ç¤º"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        metric_name = self.trials_df['metric_name'].iloc[0]
        best_idx = self.trials_df['final_metric'].idxmax()
        best_trial = self.trials_df.iloc[best_idx]
        
        print("\nğŸ† æœ€é«˜æ€§èƒ½ã®è©¦è¡Œ:")
        print(f"Trial ID: {best_trial['trial_id']}")
        print(f"{metric_name}: {best_trial['final_metric']:.4f}")
        print(f"å­¦ç¿’ç‡: {best_trial['learning_rate']:.6f}")
        print(f"ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {best_trial['optimizer']}")
        print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {best_trial['batch_size']}")
        print(f"ã‚¨ãƒãƒƒã‚¯æ•°: {best_trial['max_epochs']}")
        print(f"ãƒ¢ãƒ‡ãƒ«: {best_trial['model_name']}")
        print(f"å­¦ç¿’æ™‚é–“: {best_trial['training_time_s']:.2f}ç§’")
        print(f"ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°: {best_trial['final_iteration']}")
        
        return best_trial
    
    def get_worst_trial(self):
        """æœ€ä½æ€§èƒ½ã®è©¦è¡Œã‚’è¡¨ç¤º"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        metric_name = self.trials_df['metric_name'].iloc[0]
        worst_idx = self.trials_df['final_metric'].idxmin()
        worst_trial = self.trials_df.iloc[worst_idx]
        
        print("\nğŸ’” æœ€ä½æ€§èƒ½ã®è©¦è¡Œ:")
        print(f"Trial ID: {worst_trial['trial_id']}")
        print(f"{metric_name}: {worst_trial['final_metric']:.4f}")
        print(f"å­¦ç¿’ç‡: {worst_trial['learning_rate']:.6f}")
        print(f"ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {worst_trial['optimizer']}")
        print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {worst_trial['batch_size']}")
        print(f"ãƒ¢ãƒ‡ãƒ«: {worst_trial['model_name']}")
        
        return worst_trial
    
    def analyze_hyperparameters(self):
        """ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŠ¹æœã‚’åˆ†æ"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        print("\nğŸ“ˆ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹æœåˆ†æ:")
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶åˆ¥åˆ†æ
        print("\nğŸ”„ ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶åˆ¥æ€§èƒ½:")
        optimizer_stats = self.trials_df.groupby('optimizer')['final_metric'].agg([
            'count', 'mean', 'std', 'min', 'max'
        ]).round(4)
        print(optimizer_stats)
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥åˆ†æ
        print("\nğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥æ€§èƒ½:")
        batch_stats = self.trials_df.groupby('batch_size')['final_metric'].agg([
            'count', 'mean', 'std', 'min', 'max'
        ]).round(4)
        print(batch_stats)
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥åˆ†æ
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«åˆ¥æ€§èƒ½:")
        model_stats = self.trials_df.groupby('model_name')['final_metric'].agg([
            'count', 'mean', 'std', 'min', 'max'
        ]).round(4)
        print(model_stats)
        
        # å­¦ç¿’ç‡ã¨æ€§èƒ½ã®ç›¸é–¢
        correlation = self.trials_df['learning_rate'].corr(self.trials_df['final_metric'])
        print(f"\nğŸ“Š å­¦ç¿’ç‡ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›¸é–¢ä¿‚æ•°: {correlation:.4f}")
        
        # ã‚¨ãƒãƒƒã‚¯æ•°ã¨æ€§èƒ½ã®ç›¸é–¢
        correlation_epochs = self.trials_df['max_epochs'].corr(self.trials_df['final_metric'])
        print(f"ğŸ“Š ã‚¨ãƒãƒƒã‚¯æ•°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›¸é–¢ä¿‚æ•°: {correlation_epochs:.4f}")
    
    def get_top_trials(self, n=5):
        """ä¸Šä½nå€‹ã®è©¦è¡Œã‚’è¡¨ç¤º"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        metric_name = self.trials_df['metric_name'].iloc[0]
        top_trials = self.trials_df.nlargest(n, 'final_metric')
        
        print(f"\nğŸ… ä¸Šä½{n}ä½ã®è©¦è¡Œ:")
        for i, (_, trial) in enumerate(top_trials.iterrows(), 1):
            print(f"\n{i}ä½: Trial {trial['trial_id']}")
            print(f"  {metric_name}: {trial['final_metric']:.4f}")
            print(f"  å­¦ç¿’ç‡: {trial['learning_rate']:.6f}")
            print(f"  ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {trial['optimizer']}")
            print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {trial['batch_size']}")
            print(f"  ãƒ¢ãƒ‡ãƒ«: {trial['model_name']}")
        
        return top_trials
    
    def analyze_convergence(self):
        """åæŸæ€§ã‚’åˆ†æ"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        print("\nğŸ“‰ åæŸæ€§åˆ†æ:")
        
        # æ—©æœŸçµ‚äº†ã—ãŸè©¦è¡Œã®åˆ†æ
        completed_trials = self.trials_df[self.trials_df['status'] == 'TERMINATED']
        print(f"å®Œäº†ã—ãŸè©¦è¡Œæ•°: {len(completed_trials)}")
        
        # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°ã®çµ±è¨ˆ
        print(f"\nã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°çµ±è¨ˆ:")
        print(f"  å¹³å‡: {self.trials_df['final_iteration'].mean():.1f}")
        print(f"  æœ€å¤§: {self.trials_df['final_iteration'].max()}")
        print(f"  æœ€å°: {self.trials_df['final_iteration'].min()}")
        
        # æ€§èƒ½å‘ä¸Šã®å‚¾å‘
        trials_by_time = self.trials_df.sort_values('training_time_s')
        running_max = trials_by_time['final_metric'].expanding().max()
        improvement_rate = (running_max.iloc[-1] - running_max.iloc[0]) / len(trials_by_time)
        print(f"\næ€§èƒ½å‘ä¸Šç‡ï¼ˆè©¦è¡Œã‚ãŸã‚Šï¼‰: {improvement_rate:.6f}")
    
    def save_to_csv(self, output_path="hpo_results.csv"):
        """çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        self.trials_df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"âœ“ çµæœã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    def get_all_trials(self):
        """å…¨è©¦è¡Œã®DataFrameã‚’è¡¨ç¤º"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        print("\nğŸ“‹ å…¨è©¦è¡Œãƒ‡ãƒ¼ã‚¿:")
        # é‡è¦ãªåˆ—ã®ã¿ã‚’è¡¨ç¤º
        display_cols = ['trial_id', 'final_metric', 'learning_rate', 'optimizer', 
                       'batch_size', 'model_name', 'training_time_s']
        print(self.trials_df[display_cols].to_string(index=False))
        
        return self.trials_df
    
    def create_insights_report(self):
        """æ´å¯Ÿãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        metric_name = self.trials_df['metric_name'].iloc[0]
        
        print("\n" + "="*60)
        print("ğŸ” HPOå®Ÿé¨“æ´å¯Ÿãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        # 1. æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›
        best_trial = self.trials_df.loc[self.trials_df['final_metric'].idxmax()]
        print(f"\nâœ¨ æ¨å¥¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  å­¦ç¿’ç‡: {best_trial['learning_rate']:.6f}")
        print(f"  ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {best_trial['optimizer']}")
        print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {best_trial['batch_size']}")
        print(f"  ãƒ¢ãƒ‡ãƒ«: {best_trial['model_name']}")
        print(f"  â†’ æœŸå¾…æ€§èƒ½: {best_trial['final_metric']:.4f} {metric_name}")
        
        # 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ¹ç‡æ€§
        efficiency = self.trials_df['final_metric'] / self.trials_df['training_time_s']
        best_efficiency_idx = efficiency.idxmax()
        efficient_trial = self.trials_df.loc[best_efficiency_idx]
        
        print(f"\nâš¡ æœ€ã‚‚åŠ¹ç‡çš„ãªè¨­å®š:")
        print(f"  Trial ID: {efficient_trial['trial_id']}")
        print(f"  {metric_name}: {efficient_trial['final_metric']:.4f}")
        print(f"  å­¦ç¿’æ™‚é–“: {efficient_trial['training_time_s']:.1f}ç§’")
        print(f"  åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢: {efficiency[best_efficiency_idx]:.6f}")
        
        # 3. æ³¨æ„ã™ã¹ãè¨­å®š
        worst_trials = self.trials_df.nsmallest(3, 'final_metric')
        print(f"\nâš ï¸  é¿ã‘ã‚‹ã¹ãè¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³:")
        common_bad_optimizers = worst_trials['optimizer'].mode()
        if len(common_bad_optimizers) > 0:
            print(f"  ä½æ€§èƒ½ã§ã‚ˆãè¦‹ã‚‰ã‚Œã‚‹ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {common_bad_optimizers[0]}")
        
        # 4. å®‰å®šæ€§åˆ†æ
        optimizer_stability = self.trials_df.groupby('optimizer')['final_metric'].std()
        most_stable = optimizer_stability.idxmin()
        print(f"\nğŸ¯ æœ€ã‚‚å®‰å®šã—ãŸã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {most_stable} (æ¨™æº–åå·®: {optimizer_stability[most_stable]:.4f})")
    
    def analyze_learning_rate_ranges(self):
        """å­¦ç¿’ç‡ç¯„å›²åˆ¥ã®æ€§èƒ½åˆ†æ"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        print("\nğŸ“ˆ å­¦ç¿’ç‡ç¯„å›²åˆ¥æ€§èƒ½åˆ†æ:")
        
        # å­¦ç¿’ç‡ã‚’ç¯„å›²åˆ¥ã«åˆ†é¡
        lr_ranges = [
            (0, 0.001, "ä½å­¦ç¿’ç‡ (< 0.001)"),
            (0.001, 0.003, "ä¸­å­¦ç¿’ç‡ (0.001-0.003)"),
            (0.003, 0.01, "é«˜å­¦ç¿’ç‡ (> 0.003)")
        ]
        
        for min_lr, max_lr, label in lr_ranges:
            range_trials = self.trials_df[
                (self.trials_df['learning_rate'] >= min_lr) & 
                (self.trials_df['learning_rate'] < max_lr)
            ]
            
            if len(range_trials) > 0:
                print(f"\n{label}:")
                print(f"  è©¦è¡Œæ•°: {len(range_trials)}")
                print(f"  å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {range_trials['final_metric'].mean():.4f}")
                print(f"  æœ€é«˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {range_trials['final_metric'].max():.4f}")
                print(f"  æœ€ä½ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {range_trials['final_metric'].min():.4f}")
    
    def compare_model_performance(self):
        """ãƒ¢ãƒ‡ãƒ«é–“ã®è©³ç´°ãªæ€§èƒ½æ¯”è¼ƒ"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        print("\nğŸ¤– ãƒ¢ãƒ‡ãƒ«è©³ç´°æ¯”è¼ƒ:")
        
        for model in self.trials_df['model_name'].unique():
            model_trials = self.trials_df[self.trials_df['model_name'] == model]
            model_short = model.split('.')[0]  # ãƒ¢ãƒ‡ãƒ«åã‚’çŸ­ç¸®
            
            print(f"\n{model_short}:")
            print(f"  è©¦è¡Œæ•°: {len(model_trials)}")
            print(f"  å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {model_trials['final_metric'].mean():.4f}")
            print(f"  æœ€é«˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {model_trials['final_metric'].max():.4f}")
            print(f"  å¹³å‡å­¦ç¿’æ™‚é–“: {model_trials['training_time_s'].mean():.2f}ç§’")
            print(f"  æœ€é©å­¦ç¿’ç‡: {model_trials.loc[model_trials['final_metric'].idxmax(), 'learning_rate']:.6f}")
    
    def generate_executive_summary(self):
        """ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        if self.trials_df is None:
            print("ã¾ãšextract_trials_data()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return
        
        metric_name = self.trials_df['metric_name'].iloc[0]
        
        print("\n" + "="*60)
        print("ğŸ“‹ ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼")
        print("="*60)
        
        best_trial = self.trials_df.loc[self.trials_df['final_metric'].idxmax()]
        
        print(f"\nğŸ¯ ã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°:")
        print(f"  â€¢ æœ€é«˜æ€§èƒ½: {best_trial['final_metric']:.4f} {metric_name}")
        print(f"  â€¢ æœ€é©å­¦ç¿’ç‡: {best_trial['learning_rate']:.6f}")
        print(f"  â€¢ æ¨å¥¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {best_trial['optimizer']}")
        print(f"  â€¢ æ¨å¥¨ãƒãƒƒãƒã‚µã‚¤ã‚º: {best_trial['batch_size']}")
        print(f"  â€¢ æœ€é©ãƒ¢ãƒ‡ãƒ«: {best_trial['model_name'].split('.')[0]}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†å¸ƒ
        high_perf_threshold = 0.9 if 'roc_auc' in metric_name else 0.9
        low_perf_threshold = 0.5 if 'roc_auc' in metric_name else 0.5
        
        high_perf_count = len(self.trials_df[self.trials_df['final_metric'] > high_perf_threshold])
        low_perf_count = len(self.trials_df[self.trials_df['final_metric'] < low_perf_threshold])
        
        print(f"\nğŸ“Š æ€§èƒ½åˆ†å¸ƒ:")
        print(f"  â€¢ é«˜æ€§èƒ½è©¦è¡Œ ({metric_name} > {high_perf_threshold}): {high_perf_count}/{len(self.trials_df)} ({high_perf_count/len(self.trials_df)*100:.1f}%)")
        print(f"  â€¢ ä½æ€§èƒ½è©¦è¡Œ ({metric_name} < {low_perf_threshold}): {low_perf_count}/{len(self.trials_df)} ({low_perf_count/len(self.trials_df)*100:.1f}%)")
        
        # åŠ¹ç‡æ€§
        efficiency = self.trials_df['final_metric'] / self.trials_df['training_time_s']
        best_efficiency_trial = self.trials_df.loc[efficiency.idxmax()]
        
        print(f"\nâš¡ åŠ¹ç‡æ€§:")
        print(f"  â€¢ æœ€åŠ¹ç‡è©¦è¡Œ: {best_efficiency_trial['trial_id']}")
        print(f"  â€¢ åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢: {efficiency.max():.6f}")
        print(f"  â€¢ å¹³å‡å­¦ç¿’æ™‚é–“: {self.trials_df['training_time_s'].mean():.2f}ç§’")
        
        print(f"\nğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print(f"  1. å­¦ç¿’ç‡ {best_trial['learning_rate']:.6f} ã‚’åŸºæº–å€¤ã¨ã—ã¦ä½¿ç”¨")
        print(f"  2. {best_trial['optimizer']} ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã‚’æ¡ç”¨æ¨å¥¨")
        print(f"  3. ãƒãƒƒãƒã‚µã‚¤ã‚º {best_trial['batch_size']} ã§æœ¬ç•ªå®Ÿè£…")
        print(f"  4. {best_trial['model_name'].split('.')[0]} ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ")
        print(f"  5. å­¦ç¿’æ™‚é–“åŠ¹ç‡ã‚’é‡è¦–ã™ã‚‹å ´åˆã¯ Trial {best_efficiency_trial['trial_id']} ã®è¨­å®šã‚’æ¤œè¨")


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    import argparse
    import sys
    import os
    import glob
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è¨­å®š
    parser = argparse.ArgumentParser(description='HPOå®Ÿé¨“çµæœã‚’åˆ†æã™ã‚‹ãƒ„ãƒ¼ãƒ«')
    parser.add_argument('-m', '--model-dir', 
                       required=True,
                       help='å®Ÿé¨“çµæœãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹')
    parser.add_argument('--output', '-o', 
                       default='hpo_analysis_results.csv',
                       help='å‡ºåŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: hpo_analysis_results.csv)')
    parser.add_argument('--top-n', '-n', 
                       type=int, default=5,
                       help='ä¸Šä½ä½•ä½ã¾ã§è¡¨ç¤ºã™ã‚‹ã‹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)')
    parser.add_argument('--quick', '-q', 
                       action='store_true',
                       help='ç°¡æ˜“åˆ†æãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚µãƒãƒªãƒ¼ã¨ãƒ™ã‚¹ãƒˆè©¦è¡Œã®ã¿è¡¨ç¤ºï¼‰')
    
    args = parser.parse_args()
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨ãƒã‚§ãƒƒã‚¯
    if not os.path.exists(args.model_dir):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{args.model_dir}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    if not os.path.isdir(args.model_dir):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: '{args.model_dir}' ã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    
    # experiment_state-*.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    json_pattern = os.path.join(args.model_dir, "experiment_state-*.json")
    json_files = glob.glob(json_pattern)
    
    if len(json_files) == 0:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: '{args.model_dir}' å†…ã«experiment_state-*.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        sys.exit(1)
    elif len(json_files) > 1:
        print(f"âš ï¸  è­¦å‘Š: è¤‡æ•°ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:")
        for f in json_files:
            print(f"  - {f}")
        print(f"æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™: {max(json_files, key=os.path.getmtime)}")
        json_file = max(json_files, key=os.path.getmtime)
    else:
        json_file = json_files[0]
    
    print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {args.model_dir}")
    print(f"ğŸ“„ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {json_file}")
    print(f"ğŸ“Š å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.output}")
    print("-" * 50)
    
    # HPODataExtractorã®å®Ÿè¡Œ
    extractor = HPODataExtractor(json_file)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨æŠ½å‡º
    if extractor.load_data():
        df = extractor.extract_trials_data()
        
        if df is None or len(df) == 0:
            print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)
        
        # åˆ†æã®å®Ÿè¡Œ
        if args.quick:
            # ç°¡æ˜“åˆ†æãƒ¢ãƒ¼ãƒ‰
            print("\nğŸš€ ç°¡æ˜“åˆ†æãƒ¢ãƒ¼ãƒ‰")
            extractor.get_summary()
            extractor.get_best_trial()
        else:
            # å®Œå…¨åˆ†æãƒ¢ãƒ¼ãƒ‰
            print("\nğŸ”¬ å®Œå…¨åˆ†æãƒ¢ãƒ¼ãƒ‰")
            extractor.get_summary()
            extractor.get_best_trial()
            extractor.get_worst_trial()
            extractor.analyze_hyperparameters()
            extractor.analyze_convergence()
            extractor.get_top_trials(args.top_n)
            extractor.analyze_learning_rate_ranges()
            extractor.compare_model_performance()
            extractor.create_insights_report()
            extractor.generate_executive_summary()
        
        # çµæœã‚’CSVã«ä¿å­˜
        extractor.save_to_csv(args.output)
        
        print(f"\nâœ… åˆ†æå®Œäº†ï¼æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã¯ {len(df)} è¡Œã§ã™ã€‚")
        print(f"ğŸ“ è©³ç´°ãƒ‡ãƒ¼ã‚¿ã¯ {args.output} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
    else:
        print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        sys.exit(1)